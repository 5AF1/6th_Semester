{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment Classification with Naive Bayes Classifier","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNu6vNIj8kNoSQdPRHV2kcM"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"iEpkKsMe_4Wz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608890001114,"user_tz":-360,"elapsed":779,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}},"outputId":"3baaca8d-5f6a-46b2-feec-74ac302bf5aa"},"source":["from utils import process_tweet, lookup\n","import pdb\n","from nltk.corpus import stopwords, twitter_samples\n","import numpy as np\n","import pandas as pd\n","import nltk\n","import string\n","from nltk.tokenize import TweetTokenizer\n","from os import getcwd\n","\n","\n","nltk.download('stopwords')"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Safi\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"auFTZeGOC08e","executionInfo":{"status":"ok","timestamp":1608890005195,"user_tz":-360,"elapsed":1415,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}}},"source":["train_df = pd.read_csv('train.csv')\n","test_df = pd.read_csv('test.csv')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OzwMhBzv9VrE","executionInfo":{"status":"ok","timestamp":1608890052714,"user_tz":-360,"elapsed":796,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}},"outputId":"daac8832-10e2-44a0-9acd-903bd4a6632f"},"source":["train_df"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                      x    y\n","0     #ClimateChange #CC California's powerful and i...  0.0\n","1                 @ohkaibaeks I only have 1 though!! :(  0.0\n","2     @MsKristinKreuk Hugs ang Kisses from the phili...  1.0\n","3                             @joohyunvrl definitely :D  1.0\n","4     I will fulfil all your fantasies :) ðŸ‘‰ http://t...  1.0\n","...                                                 ...  ...\n","7997  Views from our office are pretty awesome and t...  1.0\n","7998                    Hallo :) http://t.co/ZuvvBbxiMR  1.0\n","7999  Such a stressful and upsetting day yesterday, ...  0.0\n","8000  @NintendoUK @InTheLittleWood I want his game!!...  0.0\n","8001  @daddyksoo I JUST SAW YOUR MENTION OMG SORRY :...  0.0\n","\n","[8002 rows x 2 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#ClimateChange #CC California's powerful and i...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@ohkaibaeks I only have 1 though!! :(</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@MsKristinKreuk Hugs ang Kisses from the phili...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@joohyunvrl definitely :D</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I will fulfil all your fantasies :) ðŸ‘‰ http://t...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>Views from our office are pretty awesome and t...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>Hallo :) http://t.co/ZuvvBbxiMR</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7999</th>\n      <td>Such a stressful and upsetting day yesterday, ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8000</th>\n      <td>@NintendoUK @InTheLittleWood I want his game!!...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8001</th>\n      <td>@daddyksoo I JUST SAW YOUR MENTION OMG SORRY :...</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8002 rows Ã— 2 columns</p>\n</div>"},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"Rdn00mJdFNmU"},"source":["train_df.head(20)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                    x    y\n","0   #ClimateChange #CC California's powerful and i...  0.0\n","1               @ohkaibaeks I only have 1 though!! :(  0.0\n","2   @MsKristinKreuk Hugs ang Kisses from the phili...  1.0\n","3                           @joohyunvrl definitely :D  1.0\n","4   I will fulfil all your fantasies :) ðŸ‘‰ http://t...  1.0\n","5   Sometimes it be's like that, yo. Follow someon...  0.0\n","6   @Dat_NiggaCarlos :((( it's not like a fersuree...  0.0\n","7                            saturday classes :( fuck  0.0\n","8   There's nothing as cool as being totally over ...  1.0\n","9                            Bantime: -1 :) #fail2ban  1.0\n","10                 @nongardener Thank you Pauline! :)  1.0\n","11      they love you too :)  https://t.co/FePD5DUbNk  1.0\n","12  @ileana_official Its not fair...... what about...  1.0\n","13  @theroiceorven  follow @jnlazts &amp; http://t...  1.0\n","14              @SlightlySalty_ is a good witness :-)  1.0\n","15  @BRose_24 haha I feel you. Maybe that's why I ...  1.0\n","16  Smart, handshome, cute juga :D https://t.co/7D...  1.0\n","17         lots of fun!!! :)))) newyork magic? lollll  1.0\n","18  @jackjonestv I was 121st in yesterday's Rt for...  0.0\n","19  @AnnieCushing nice seeing it in Spanish... Wel...  1.0"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#ClimateChange #CC California's powerful and i...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@ohkaibaeks I only have 1 though!! :(</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@MsKristinKreuk Hugs ang Kisses from the phili...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@joohyunvrl definitely :D</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I will fulfil all your fantasies :) ðŸ‘‰ http://t...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Sometimes it be's like that, yo. Follow someon...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>@Dat_NiggaCarlos :((( it's not like a fersuree...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>saturday classes :( fuck</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>There's nothing as cool as being totally over ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Bantime: -1 :) #fail2ban</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>@nongardener Thank you Pauline! :)</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>they love you too :)  https://t.co/FePD5DUbNk</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>@ileana_official Its not fair...... what about...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>@theroiceorven  follow @jnlazts &amp;amp; http://t...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>@SlightlySalty_ is a good witness :-)</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>@BRose_24 haha I feel you. Maybe that's why I ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Smart, handshome, cute juga :D https://t.co/7D...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>lots of fun!!! :)))) newyork magic? lollll</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>@jackjonestv I was 121st in yesterday's Rt for...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>@AnnieCushing nice seeing it in Spanish... Wel...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"cEhoJVqOFW8T","executionInfo":{"status":"ok","timestamp":1608890073890,"user_tz":-360,"elapsed":780,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}}},"source":["train_x = list(train_df.x.values)\n","train_y = list(train_df.y.values)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ugzs5T-9GHT-"},"source":["# Part 1:  Implementing your helper functions\n","\n","To help train your naive bayes model, you will need to build a dictionary where the keys are a (word, label) tuple and the values are the corresponding frequency.  Note that the labels we'll use here are 1 for positive and 0 for negative.\n","\n","A  `lookup()` helper function is here that `freqs` dictionary, a word, and a label (1 or 0) and returns the number of times that word and label tuple appears in the collection of tweets.\n","\n","For example: given a list of tweets `[\"i am rather excited\", \"you are rather happy\"]` and the label 1, the function will return a dictionary that contains the following key-value pairs:\n","\n","{\n","    (\"rather\", 1): 2\n","    (\"happi\", 1) : 1\n","    (\"excit\", 1) : 1\n","}\n","\n","\n","#### Instructions\n","Create a function `count_tweets()` that takes a list of tweets as input, cleans all of them, and returns a dictionary.\n","- The key in the dictionary is a tuple containing the stemmed word and its class label, e.g. (\"happi\",1).\n","- The value the number of times this word appears in the given collection of tweets (an integer)."]},{"cell_type":"code","metadata":{"id":"W3sibH9mFf_Z","executionInfo":{"status":"ok","timestamp":1608890430785,"user_tz":-360,"elapsed":774,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}}},"source":["def count_tweets(result, tweets, ys):\n","    '''\n","    Input:\n","        tweets: a list of tweets\n","        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n","    Output:\n","        result: a dictionary mapping each pair to its frequency\n","    '''\n","    # result = {}\n","    for tweet, y in zip(tweets, ys):\n","        for word in process_tweet(tweet):\n","            # define the key\n","            pair = (word,y) # ('happy', 1) key in dictionary\n","            # if the key exists in the dictionary, increment the count\n","            result[pair] = result.get(pair, 0) +1\n","    return result"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IFtSWF9tFhdN","executionInfo":{"status":"ok","timestamp":1608890431218,"user_tz":-360,"elapsed":1041,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}},"outputId":"72dec80b-be4d-462c-c1c4-fbd9be1a926e"},"source":["# Testing your function\n","result = {}\n","tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired', ' I am so happy', 'I am not happy']\n","ys = [1, 0, 0, 0, 0, 1, 0]\n","count_tweets(result, tweets, ys)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{('happi', 1): 2,\n"," ('trick', 0): 1,\n"," ('sad', 0): 1,\n"," ('tire', 0): 2,\n"," ('happi', 0): 1}"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"ZYNtr3TZH7FQ"},"source":["Part 2: Training The Baysian Classifier\n","## Prior and Logprior\n","\n","The prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n","\n","The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\n","We can take the log of the prior to rescale it, and we'll call this the logprior\n","\n","$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n","\n","Note that $log(\\frac{A}{B})$ is the same as $log(A) - log(B)$.  So the logprior can also be calculated as the difference between two logs:\n","\n","$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$"]},{"cell_type":"markdown","metadata":{"id":"afxGqKiHIfMJ"},"source":["## Positive and Negative Probability of a Word\n","To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n","\n","- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n","- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n","- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n","\n","We'll use these to compute the positive and negative probability for a specific word using this formula:\n","\n","$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n","$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n","\n","Notice that we add the \"+1\" in the numerator for additive smoothing.  This [wiki article](https://en.wikipedia.org/wiki/Additive_smoothing) explains more about additive smoothing."]},{"cell_type":"markdown","metadata":{"id":"_RVw3g3WI5Ky"},"source":["## Log likelihood\n","To compute the loglikelihood of that very same word, we can implement the following equations:\n","\n","$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"]},{"cell_type":"markdown","metadata":{"id":"vFChGefbJgZj"},"source":["## Create `freqs` dictionary\n","- Given your `count_tweets()` function, you can compute a dictionary called `freqs` that contains all the frequencies.\n","- In this `freqs` dictionary, the key is the tuple (word, label)\n","- The value is the number of times it has appeared.\n","\n","We will use this dictionary in several parts of this assignment."]},{"cell_type":"code","metadata":{"id":"r0cfl00GHnzU","executionInfo":{"status":"ok","timestamp":1608890443149,"user_tz":-360,"elapsed":3707,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}}},"source":["# Build the freqs dictionary for later uses\n","freqs = count_tweets({}, train_x, train_y)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IOjIefgiKcnP"},"source":["## Steps to Train Baysian Classifier\n","Given a freqs dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), implement a naive bayes classifier.\n","\n","##### Calculate $V$\n","- You can then compute the number of unique words that appear in the `freqs` dictionary to get your $V$ (you can use the `set` function).\n","\n","##### Calculate $freq_{pos}$ and $freq_{neg}$\n","- Using your `freqs` dictionary, you can compute the positive and negative frequency of each word $freq_{pos}$ and $freq_{neg}$.\n","\n","##### Calculate $N_{pos}$ and $N_{neg}$\n","- Using `freqs` dictionary, you can also compute the total number of positive words and total number of negative words $N_{pos}$ and $N_{neg}$.\n","\n","##### Calculate $D$, $D_{pos}$, $D_{neg}$\n","- Using the `train_y` input list of labels, calculate the number of documents (tweets) $D$, as well as the number of positive documents (tweets) $D_{pos}$ and number of negative documents (tweets) $D_{neg}$.\n","- Calculate the probability that a document (tweet) is positive $P(D_{pos})$, and the probability that a document (tweet) is negative $P(D_{neg})$\n","\n","##### Calculate the logprior\n","- the logprior is $log(D_{pos}) - log(D_{neg})$\n","\n","##### Calculate log likelihood\n","- Finally, you can iterate over each word in the vocabulary, use your `lookup` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n","- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using equations 4 & 5.\n","\n","$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n","$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n","\n","**Note:** We'll use a dictionary to store the log likelihoods for each word.  The key is the word, the value is the log likelihood of that word).\n","\n","- You can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$."]},{"cell_type":"code","metadata":{"id":"zfmetSLDJopT","executionInfo":{"status":"ok","timestamp":1608890729707,"user_tz":-360,"elapsed":817,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}}},"source":["def train_naive_bayes(freqs, train_x, train_y):\n","    '''\n","    Input:\n","        freqs: dictionary from (word, label) to how often the word appears\n","        train_x: a list of tweets\n","        train_y: a list of labels correponding to the tweets (0,1)\n","    Output:\n","        logprior: the log prior. (equation 3 above)\n","        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n","    '''\n","    loglikelihood = {}\n","    logprior = 0\n","\n","    # calculate V, the number of unique words in the vocabulary\n","    vocab = set([pair[0] for pair in freqs.keys()]) \n","    V = len(vocab)\n","\n","    # calculate N_pos and N_neg\n","    N_pos = N_neg = 0\n","    for pair in freqs.keys():\n","        # if the label is positive (greater than zero)\n","        if pair[1]:\n","            N_pos += freqs[pair] # Increment the number of positive words by the count for this (word, label) pair\n","        else:\n","            N_neg += freqs[pair]\n","\n","    \n","    D = len(train_y) # Calculate D, the number of documents\n","    D_pos = np.sum(train_df.y==1) # Calculate D_pos and D_neg, the number of positive and negative documents\n","    D_neg = np.sum(train_df.y==0)\n","    logprior = np.log(D_pos) - np.log(D_neg)     # Calculate logprior\n","\n","    # For each word in the vocabulary...\n","    for word in vocab:\n","        # get the positive and negative frequency of the word\n","        freq_pos = lookup(freqs,word,1) \n","        freq_neg = lookup(freqs,word,0)\n","\n","        # calculate the probability that each word is positive, and negative\n","        p_w_pos = (freq_pos + 1) / (N_pos + V)\n","        p_w_neg = (freq_neg + 1) / (N_neg + V)\n","\n","        # calculate the log likelihood of the word\n","        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n","\n","    ### END CODE HERE ###\n","\n","    return logprior, loglikelihood"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4NoXLI5Mlcw","executionInfo":{"status":"ok","timestamp":1608890732155,"user_tz":-360,"elapsed":794,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}},"outputId":"4b919247-3b9c-4786-f76f-b60a2f97f1ba"},"source":["logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n","print(logprior)\n","print(len(loglikelihood))"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n9085\n"]}]},{"cell_type":"code","metadata":{"id":"vShMEpukPm9z"},"source":["loglikelihood['happi']"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.148320878725993"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"1khjrpYnQDb4"},"source":["Part 3: Testing The classifier: \n","Now that we have the `logprior` and `loglikelihood`, we can test the naive bayes function by making predicting on some tweets!\n","\n","#### Implement `naive_bayes_predict`\n","**Instructions**:\n","Implement the `naive_bayes_predict` function to make predictions on tweets.\n","* The function takes in the `tweet`, `logprior`, `loglikelihood`.\n","* It returns the probability that the tweet belongs to the positive or negative class.\n","* For each tweet, sum up loglikelihoods of each word in the tweet.\n","* Also add the logprior to this sum to get the predicted sentiment of that tweet.\n","\n","$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n","\n","#### Note\n","Note we calculate the prior from the training data, and that the training data is evenly split between positive and negative labels (4000 positive and 4000 negative tweets).  This means that the ratio of positive to negative 1, and the logprior is 0.\n","\n","The value of 0.0 means that when we add the logprior to the log likelihood, we're just adding zero to the log likelihood.  However, please remember to include the logprior, because whenever the data is not perfectly balanced, the logprior will be a non-zero value."]},{"cell_type":"code","metadata":{"id":"qnvv8aBYMn8m","executionInfo":{"status":"ok","timestamp":1608890853358,"user_tz":-360,"elapsed":829,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}}},"source":["def naive_bayes_predict(tweet, logprior, loglikelihood):\n","    '''\n","    Input:\n","        tweet: a string\n","        logprior: a number\n","        loglikelihood: a dictionary of words mapping to numbers\n","    Output:\n","        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n","    '''\n","    word_l = process_tweet(tweet) #tokenizing the tweet and processing\n","\n","    p = 0  # initialize probability to zero\n","    p += logprior # add the logprior\n","\n","    for word in word_l:\n","        if word in loglikelihood:   # check if the word exists in the loglikelihood dictionary\n","            p += loglikelihood[word] # add the log likelihood of that word to the probability\n","    return p"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zX21iKpJGeOD","executionInfo":{"status":"ok","timestamp":1608634810247,"user_tz":-360,"elapsed":1071,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}},"outputId":"c9fa96c0-f138-43e6-a30e-7389d397ac9d"},"source":["loglikelihood['smile'], loglikelihood['joy'], loglikelihood['sad']"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1.5737794405738943, 0.2928455951118301, -2.8181975249531552)"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oEiAmiJOHJrh","executionInfo":{"status":"ok","timestamp":1608634927398,"user_tz":-360,"elapsed":1154,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}},"outputId":"a1ca9b12-3912-4f32-daa9-61c4333a9004"},"source":["loglikelihood[':)'], loglikelihood[':(']"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6.860572321269977, -7.508000022574026)"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pntsR43QMy7g","executionInfo":{"status":"ok","timestamp":1608891181513,"user_tz":-360,"elapsed":973,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}},"outputId":"c558daef-7c1f-49b0-d00a-101f448363c5"},"source":["# Experiment with your own tweet.\n","my_tweet = 'Jannet Smiled with fear and cried, broke down'\n","p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n","print('The expected output is', p)"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["The expected output is -1.2476130643911365\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxQkX5bZYGnO","executionInfo":{"status":"ok","timestamp":1608891183044,"user_tz":-360,"elapsed":1058,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}},"outputId":"c2aefa76-da81-4cbb-d8e5-c57f631a8aa7"},"source":["words = process_tweet(my_tweet)\n","words\n","\n","for word in words:\n","    if word in loglikelihood:\n","        print(f'{word} Likelihood : {loglikelihood[word]} ')\n","    else:\n","        print(f'{word} not found in training corpus!')\n"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["jannet not found in training corpus!\nsmile Likelihood : 1.5737794405738943 \nfear Likelihood : 0.005163522660049284 \ncri Likelihood : -1.5789565817897613 \nbroke Likelihood : -1.2475994458353188 \n"]}]},{"cell_type":"code","metadata":{"id":"1DUxZkW2Wnbh","executionInfo":{"status":"ok","timestamp":1608891243121,"user_tz":-360,"elapsed":791,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}}},"source":["test_df = pd.read_csv('test.csv')"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"pE4kOu6qHb9d","executionInfo":{"status":"ok","timestamp":1608891243613,"user_tz":-360,"elapsed":1087,"user":{"displayName":"Md. Redwan Karim Sony, Lecturer, CSE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_ehXyeemZBv77YyrTHljAkCyNIj_APibTTUvrjQk=s64","userId":"11630697472793854312"}},"outputId":"10b37b5b-1484-4f49-8e5c-48c968e3684d"},"source":["test_df.head()"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   x    y\n","0  @V4Violetta Or that. I guess I need to build m...  1.0\n","1               @sennicka don't over engineer it. :)  1.0\n","2  @ellieharveyy it probably your fault he lost h...  0.0\n","3              Good morning Kimmy :) @KimberlyKWyatt  1.0\n","4  @RockMyWedding @SouthFarm1 @JohnHopePhoto @Mir...  1.0"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@V4Violetta Or that. I guess I need to build m...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@sennicka don't over engineer it. :)</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@ellieharveyy it probably your fault he lost h...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Good morning Kimmy :) @KimberlyKWyatt</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@RockMyWedding @SouthFarm1 @JohnHopePhoto @Mir...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"rARY_RiFHlXQ"},"source":["Calculate the following performance parameters from test.csv data:\n","* TP, FP, TN, FN rates\n","* Accuracy\n","* Precision\n","* F1 score"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["y_true = list(test_df.y.values)\n","test_x = list(test_df.x.values)\n","y_pred = []"]},{"cell_type":"code","metadata":{"id":"-FKCXyv-HeLI"},"source":["y_true, y_pred"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  nan,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  1.0,\n","  1.0,\n","  1.0,\n","  0.0,\n","  0.0,\n","  ...],\n"," [])"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9945027486256871\nPrecision: 0.995\nRecall: 0.994005994005994\nF1 score: 0.9945027486256871\n"]}],"source":["y_true = list(test_df.y.values)\n","test_x = list(test_df.x.values)\n","y_pred = []\n","for tweet in test_x:\n","    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n","    if p < 0:\n","        p = 0\n","    else:\n","        p = 1\n","    y_pred.append(p)\n","\n","TP = 0\n","TN = 0\n","FP = 0\n","FN = 0\n","\n","for i in range(len(y_pred)):\n","    if y_pred[i] == 1 and y_true[i] == 1:\n","        TP += 1\n","    elif y_pred[i] == 0 and y_true[i] == 0:\n","        TN += 1\n","    elif y_pred[i] == 1 and y_true[i] == 0:\n","        FP += 1\n","    else:\n","        FN += 1\n","\n","accuracy = (TP+TN)/(TP+FP+FN+TN)\n","precision = TP/(TP+FP)\n","recall = TP/(TP+FN)\n","\n","F1 =  2*(recall * precision) / (recall + precision)\n","\n","print(f\"Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 score: {F1}\")"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.9945027486256871, 0.995, 0.994005994005994, 0.9945027486256871)"]},"metadata":{},"execution_count":56}],"source":["accuracy,precision,recall,F1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}